{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from tensorflow.keras.models import Sequential, Model, Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Input, Dense, Add\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the Flickr8k images and caption files\n",
    "dataset_image_path =\"flickr8k/Images/\"\n",
    "dataset_text_path  =\"flickr8k/captions.txt\" \n",
    "# Wanted shape for images\n",
    "wanted_shape = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the text dataset corresponding to images\n",
    "train, infer = False, True\n",
    "\n",
    "if train:\n",
    "    df_texts = pd.read_csv(dataset_text_path, sep=\",\") #[\"image\",\"caption\"] \n",
    "elif infer:\n",
    "    df_texts = pd.read_csv(\"df_texts.csv\", index_col=0) # [\"image\",\"caption\",\"cleaned\",\"cleaned_tokenized\"]\n",
    "    print(\"df_texts loaded\")\n",
    "    print(df_texts.head())\n",
    "\n",
    "n_img = df_texts.count()/5 # 40455/5 \n",
    "unique_img = pd.unique(df_texts[\"image\"])# 8091 unique images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing images with pretrained VGG16 : FEATURE MAPS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape=wanted_shape, pooling=None, classes=1000\n",
    ")\n",
    "# Feature extraction\n",
    "vgg_model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output) #end the mod√®le with a 4096 feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "charge_image, one_by_one = False, False# false to gain time when testing other parts\n",
    "# To obtain the feature maps\n",
    "if train :\n",
    "    if charge_image:\n",
    "        feature_maps = np.array([vgg_model.predict(load_img_from_ds(unique_img[i])) for i in range(len(unique_img))])\n",
    "        print(f\"Shape des fm {feature_maps.shape}\")\n",
    "    elif one_by_one:\n",
    "        feature_maps=[]\n",
    "        for i in range(len(unique_img)):\n",
    "            if i!=0:\n",
    "                print(f\"{i}/{len(unique_img)} - time elapsed :{time.time()-a}\")\n",
    "            else:\n",
    "                print(f\"{i}/{len(unique_img)}\")\n",
    "            a=time.time()\n",
    "            img = load_img_from_ds(unique_img[i])\n",
    "            feature_map = vgg_model.predict(img)\n",
    "            feature_maps.append(feature_map)\n",
    "        feature_maps=np.array(feature_maps)\n",
    "        #save to csv\n",
    "        feature_maps_sav=feature_maps[:,0,:]\n",
    "        df_fm = pd.DataFrame(feature_maps_sav)\n",
    "        df_fm.to_csv(\"image_feature_maps.csv\")\n",
    "\n",
    "elif infer:\n",
    "    df_fm = pd.read_csv(\"image_feature_maps.csv\")\n",
    "    feature_maps = np.array(df_fm.drop([df_fm.columns[0]], axis=1))\n",
    "    print(f\"Image feature maps loaded : {feature_maps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing captions - WORD2VEC : EMBEDDINGS 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec model loaded.\n",
      "Text features loaded. (40455,)\n"
     ]
    }
   ],
   "source": [
    " if train :\n",
    "    # Text preprocessing\n",
    "    df_texts[\"cleaned\"]=[process_sentence(s) for s in df_texts[\"caption\"]]\n",
    "    df_texts[\"cleaned_tokenized\"]=[word_tokenize(w) for w in df_texts[\"cleaned\"]]\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    word2vec_model = gensim.models.Word2Vec([word_tokenize(w) for w in df_texts[\"cleaned\"]], min_count=1, size=4096)\n",
    "    word2vec_model.save(\"word2vec.model\")\n",
    "    \n",
    "    text_features = word2vec(df_texts,word2vec_model.wv)\n",
    "    np.save(\"text_feature_maps.npy\",text_features)\n",
    "    df_texts.to_csv(\"text_feature_maps.csv\")\n",
    "\n",
    "elif infer:\n",
    "    #word2vec_model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "    #vocab_size = len(word2vec_model.wv.vocab)\n",
    "    print(\"Word2vec model loaded.\")\n",
    "    text_features  = np.load(\"text_feature_maps.npy\", allow_pickle=True)\n",
    "    print(f\"Text features loaded. {text_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing total model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feature maps : (5, 4096)\n",
      "Text features : (25,)\n"
     ]
    }
   ],
   "source": [
    "if(len(feature_maps)==8091):\n",
    "    n_images_considered = 5#int(len(feature_maps)/9)\n",
    "    # Reduce memory consumption\n",
    "    feature_maps = feature_maps[:n_images_considered]\n",
    "    text_features= text_features[:n_images_considered*5]\n",
    "\n",
    "print(f\"Image feature maps : {feature_maps.shape}\\nText features : {text_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimages = feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_feature_maps(dimages):\n",
    "    Ximage = []\n",
    "    for image in dimages:\n",
    "        for i in range (5):\n",
    "            Ximage.append(image)\n",
    "    Ximage=np.array(Ximage)\n",
    "    return Ximage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images duplicated (25, 4096)\n"
     ]
    }
   ],
   "source": [
    "dfeaturemaps = multiple_feature_maps(dimages)\n",
    "print(f\"Images duplicated {dfeaturemaps.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "def split_test_val_train(df, Ntest, Nval):\n",
    "    return(df[:Ntest],\n",
    "           df[Ntest:Ntest+Nval],\n",
    "           df[Ntest+Nval:])\n",
    "    \"\"\"\n",
    "    return(df[:1000],\n",
    "           df[1000:20000],\n",
    "           df[20000:21000])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split du dataset\n",
    "prop_test, prop_val = 0.2, 0.2\n",
    "N = dfeaturemaps.shape[0]#len(df_texts[\"cleaned_tokenized\"])\n",
    "Ntest, Nval = int(N*prop_test), int(N*prop_val)\n",
    "\n",
    "Nimg = len(dimages)\n",
    "# dt = true image caption cleaned\n",
    "dt_test, dt_val, dt_train = split_test_val_train(text_features, Ntest, Nval)\n",
    "# di = true image array\n",
    "di_test, di_val, di_train = split_test_val_train(dfeaturemaps, Ntest, Nval)\n",
    "# fnm = image_name\n",
    "fnm_test, fnm_val, fnm_train = split_test_val_train(df_texts[\"image\"], Ntest, Nval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: text (15,), image (15, 4096), fnm (40445,)\n",
      " Test : text (5,), image (5, 4096), fnm (5,)\n",
      " Val  : text (5,), image (5, 4096), fnm (5,)\n"
     ]
    }
   ],
   "source": [
    "print(f\" Train: text {dt_train.shape}, image {di_train.shape}, fnm {fnm_train.shape}\")\n",
    "print(f\" Test : text {dt_test.shape}, image {di_test.shape}, fnm {fnm_test.shape}\")\n",
    "print(f\" Val  : text {dt_val.shape}, image {di_val.shape}, fnm {fnm_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringtoarray(text):\n",
    "    txt = []\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    text = text.split()\n",
    "    for i in text:\n",
    "        txt.append(float(i))\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalpreprocessing(dftext, dfimage, vocab_size):\n",
    "    print(\"# captions/images = {}\".format(len(dftext)))\n",
    "    \n",
    "    maxlen = np.max([len(text) for text in dftext])\n",
    "    Xtext, Ximage, ytext = [], [], []\n",
    "    step = 0\n",
    "    for text, image in zip(dftext, dfimage):\n",
    "        step += 1\n",
    "        # Commented because our array is a numpy ndarray dtype=float32\n",
    "        #text = stringtoarray(text)\n",
    "        for i in range(0, len(text)):\n",
    "            in_text, out_text = text[:i], text[i]\n",
    "            in_text = pad_sequences([in_text], maxlen=maxlen).flatten()\n",
    "            out_text = to_categorical(out_text, num_classes = vocab_size)\n",
    "            \n",
    "            Xtext.append(in_text)\n",
    "            Ximage.append(image)\n",
    "            ytext.append(out_text)\n",
    "    print(f\"Number of step/associated image and caption {step}\")\n",
    "    \n",
    "    Xtext = np.array(Xtext)\n",
    "    Ximage = np.array(Ximage)\n",
    "    ytext = np.array(ytext)\n",
    "    \n",
    "    return(Xtext, Ximage, ytext, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# captions/images = 15\n",
      "Number of step/associated image and caption 15\n",
      "# captions/images = 5\n",
      "Number of step/associated image and caption 5\n",
      "Vocab size 8747\n",
      "Training set : \n",
      " \tInput image : (132, 4096)\n",
      "\tInput text : (132,)\n",
      "\tOutput text : (132, 4096, 8747)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8747#len(word2vec_model.wv.vocab)\n",
    "\n",
    "Xtext_train, Ximage_train, ytext_train, maxlen = finalpreprocessing(dt_train, di_train, vocab_size) \n",
    "Xtext_val, Ximage_val, ytext_val, _ = finalpreprocessing(dt_val, di_val, vocab_size)\n",
    "\n",
    "print(f\"Vocab size {vocab_size}\")\n",
    "print(f\"Training set : \\n \\tInput image : {Ximage_train.shape}\\n\\tInput text : {Xtext_train.shape}\\n\\tOutput text : {ytext_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Xtext_train)):\n",
    "    if(1 in Xtext_train[i]):\n",
    "        print(\"Maeva a raison !\")\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"functional_3\" was not an Input tensor, it was generated by layer CompressedImageFeatures.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: CompressedImageFeatures/Relu_1:0\n",
      "WARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"functional_3\" was not an Input tensor, it was generated by layer CaptionFeatures.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: CaptionFeatures/strided_slice_7:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"InputSequence_1:0\", shape=(None, 12), dtype=float32) at layer \"embedding_1\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-35b187aaa8b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtotal_model\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_txt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m       \u001b[1;31m# Functional model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m       \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, name, trainable)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;31m#     'arguments during initialization. Got an unexpected argument:')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFunctional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0m\u001b[0;32m    191\u001b[0m         self.inputs, self.outputs)\n\u001b[0;32m    192\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m    924\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomputable_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m             raise ValueError('Graph disconnected: '\n\u001b[0m\u001b[0;32m    927\u001b[0m                              \u001b[1;34m'cannot obtain value for tensor '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                              \u001b[1;34m' at layer \"'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\". '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"InputSequence_1:0\", shape=(None, 12), dtype=float32) at layer \"embedding_1\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "dim_embedding=64\n",
    "\n",
    "# image input\n",
    "input_img = Input(shape=(Ximage_train.shape[1],), name=\"InputImage\")\n",
    "input_img = ( Dense(units=256,activation='relu',name=\"CompressedImageFeatures\"))(input_img)\n",
    "# text input\n",
    "input_txt = Input(shape=(maxlen,), name=\"InputSequence\")\n",
    "input_txt = Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\n",
    "input_txt = LSTM(units=256, activation=\"relu\", name=\"CaptionFeatures\")(input_txt)\n",
    "\n",
    "# Common part\n",
    "common = Add()([input_txt, input_img])\n",
    "common = Dense(256, activation='relu')(common)\n",
    "common = Dense(vocab_size, activation='softmax')(common)\n",
    "\n",
    "#Model\n",
    "total_model  = Model(inputs=[input_img, input_txt],outputs=common)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([Ximage_train, Xtext_train], ytext_train, epochs=5, verbose=2, batch_size=64, validation_data=([Ximage_val, Xtext_val], ytext_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"loss\", \"val_loss\"]:\n",
    "    plt.plot(hist.history[label], label=label)]\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PREDICTION\n",
    "'''\n",
    "\n",
    "# 1 couche 256 LSTM ?\n",
    "# A partir de combien de couce=hes c est ok 1 8 16 32 256 \n",
    "# Temps d entrainement : compromis \n",
    "# Voir si dimensions pas trop grandes ?\n",
    "# GRU ! :D mieux (3 params au lieu de 4)\n",
    "# simpleRNN ? \n",
    "# Etude comparative : 3 RNN (simple, LSTM, GRU & Etude de perf)\n",
    "# Limiter Dataset ! => entrainements en O(heure)\n",
    "\n",
    "#tf.keras.utils.get_file(origin=\"lien\", fname=\"nom_que_tu_veux_donner_au_fichier.zip\", extract=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
